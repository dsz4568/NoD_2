{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823cfb0a-e16b-47f9-8ba7-16821d5fd759",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Dense, BatchNormalization, Conv2D, MaxPooling2D, Flatten, \n\u001b[0;32m      6\u001b[0m                                      GRU, Dropout, Input, Embedding, LayerNormalization, \n\u001b[0;32m      7\u001b[0m                                      MultiHeadAttention, GlobalAveragePooling1D)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Dense, BatchNormalization, Conv2D, MaxPooling2D, Flatten, \n",
    "                                     GRU, Dropout, Input, Embedding, LayerNormalization, \n",
    "                                     MultiHeadAttention, GlobalAveragePooling1D)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.datasets import mnist, imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Klasyfikacja IRIS z siecią gęstą i BatchNormalization\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. Klasyfikacja IRIS z BatchNormalization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Wczytanie i przygotowanie danych\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# Kodowanie one-hot i podział danych\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizacja danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Budowa modelu\n",
    "model_iris = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(4,)),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilacja i trening modelu\n",
    "model_iris.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_iris = model_iris.fit(X_train, y_train, epochs=100, batch_size=16, \n",
    "                             validation_split=0.2, verbose=0)\n",
    "\n",
    "# Ocena modelu\n",
    "test_loss_iris, test_acc_iris = model_iris.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Dokładność modelu IRIS: {test_acc_iris:.4f}')\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Rozpoznawanie cyfr MNIST z konwolucjami 5x5\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. Klasyfikacja MNIST z jądrami 5x5\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Wczytanie i przygotowanie danych\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "X_train_mnist = X_train_mnist.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test_mnist = X_test_mnist.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "y_train_mnist = to_categorical(y_train_mnist, 10)\n",
    "y_test_mnist = to_categorical(y_test_mnist, 10)\n",
    "\n",
    "# Budowa modelu\n",
    "model_mnist = Sequential([\n",
    "    Conv2D(32, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(64, kernel_size=(5,5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilacja i trening modelu\n",
    "model_mnist.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_mnist = model_mnist.fit(X_train_mnist, y_train_mnist, epochs=5, batch_size=128, \n",
    "                               validation_split=0.1, verbose=0)\n",
    "\n",
    "# Ocena modelu\n",
    "test_loss_mnist, test_acc_mnist = model_mnist.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n",
    "print(f'Dokładność modelu MNIST: {test_acc_mnist:.4f}')\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Prognozowanie sekwencji z GRU i Dropout\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. Prognozowanie sekwencji z GRU i Dropout\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generowanie sztucznych danych\n",
    "n_samples = 10000\n",
    "seq_length = 20\n",
    "n_features = 5\n",
    "\n",
    "X_seq = np.random.randn(n_samples, seq_length, n_features)\n",
    "y_seq = np.random.randn(n_samples, 1)  # Symulacja zadania regresji\n",
    "\n",
    "# Podział danych\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Budowa modelu\n",
    "model_seq = Sequential([\n",
    "    GRU(64, input_shape=(seq_length, n_features), return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)  # Warstwa wyjściowa dla regresji\n",
    "])\n",
    "\n",
    "# Kompilacja i trening modelu\n",
    "model_seq.compile(optimizer='adam', loss='mse')\n",
    "history_seq = model_seq.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, \n",
    "                           validation_split=0.2, verbose=0)\n",
    "\n",
    "# Ocena modelu\n",
    "test_mse_seq = model_seq.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
    "print(f'Błąd MSE modelu sekwencyjnego: {test_mse_seq:.4f}')\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Analiza sentymentu IMDB z Transformer Encoder\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. Analiza sentymentu IMDB z Transformer Encoder\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Parametry\n",
    "max_features = 20000\n",
    "max_len = 200\n",
    "\n",
    "# Wczytanie i przygotowanie danych\n",
    "(X_train_imdb, y_train_imdb), (X_test_imdb, y_test_imdb) = imdb.load_data(num_words=max_features)\n",
    "X_train_imdb = pad_sequences(X_train_imdb, maxlen=max_len)\n",
    "X_test_imdb = pad_sequences(X_test_imdb, maxlen=max_len)\n",
    "\n",
    "# Funkcja budująca blok Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalizacja i mechanizm uwagi\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = tf.keras.layers.Add()([x, inputs])\n",
    "    \n",
    "    # Feed Forward Network\n",
    "    y = LayerNormalization(epsilon=1e-6)(x)\n",
    "    y = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(y)\n",
    "    y = tf.keras.layers.Dropout(dropout)(y)\n",
    "    y = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(y)\n",
    "    return tf.keras.layers.Add()([y, x])\n",
    "\n",
    "# Budowa modelu\n",
    "inputs = Input(shape=(max_len,))\n",
    "x = Embedding(max_features, 128)(inputs)\n",
    "x = transformer_encoder(x, head_size=128, num_heads=2, ff_dim=256, dropout=0.1)\n",
    "x = transformer_encoder(x, head_size=128, num_heads=2, ff_dim=256, dropout=0.1)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_imdb = Model(inputs, outputs)\n",
    "\n",
    "# Kompilacja i trening modelu\n",
    "model_imdb.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_imdb = model_imdb.fit(X_train_imdb, y_train_imdb, epochs=3, batch_size=32, \n",
    "                             validation_split=0.2, verbose=0)\n",
    "\n",
    "# Ocena modelu\n",
    "test_loss_imdb, test_acc_imdb = model_imdb.evaluate(X_test_imdb, y_test_imdb, verbose=0)\n",
    "print(f'Dokładność modelu IMDB: {test_acc_imdb:.4f}')\n",
    "\n",
    "# =============================================================================\n",
    "# Podsumowanie wyników\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Podsumowanie wszystkich modeli\")\n",
    "print(\"=\"*80)\n",
    "print(f\"1. IRIS - dokładność: {test_acc_iris:.4f}\")\n",
    "print(f\"2. MNIST - dokładność: {test_acc_mnist:.4f}\")\n",
    "print(f\"3. Sekwencyjny - MSE: {test_mse_seq:.4f}\")\n",
    "print(f\"4. IMDB - dokładność: {test_acc_imdb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da683d0-5f22-4f5d-87fa-2c9fdb764cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
