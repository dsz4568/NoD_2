{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fd11b2-6cd0-4ee4-874a-49c7c13c7108",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (D:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer, TfidfVectorizer\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD, LatentDirichletAllocation\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (D:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Krok 0: Instalacja wymaganych bibliotek\n",
    "!pip install nltk gensim wordcloud pyLDAvis --quiet\n",
    "!wget -q \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "!tar -xzf aclImdb_v1.tar.gz\n",
    "\n",
    "# Krok 1: Import bibliotek\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from gensim.models import Word2Vec\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Krok 2: Ładowanie danych\n",
    "def load_imdb_data(path):\n",
    "    reviews = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder = os.path.join(path, label)\n",
    "        for filename in os.listdir(folder):\n",
    "            with open(os.path.join(folder, filename), 'r', encoding='utf-8') as file:\n",
    "                reviews.append(file.read())\n",
    "    return reviews[:1000]  # Ograniczamy do 1000 recenzji\n",
    "\n",
    "print(\"Ładowanie danych...\")\n",
    "texts = load_imdb_data('aclImdb/train')\n",
    "print(f\"Załadowano {len(texts)} recenzji\")\n",
    "\n",
    "# Krok 3: Preprocessing tekstu\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8', errors='replace')\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "print(\"Przetwarzanie tekstów...\")\n",
    "tokenized_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Krok 4: Wektoryzacja\n",
    "print(\"Wektoryzacja...\")\n",
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "bow_matrix = vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_texts])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_texts])\n",
    "\n",
    "# Krok 5: Word2Vec\n",
    "print(\"Trenowanie Word2Vec...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Testowanie podobieństwa\n",
    "print(\"\\nPodobieństwa słów:\")\n",
    "test_words = ['movie', 'actor', 'horrible', 'excellent', 'plot']\n",
    "for word in test_words:\n",
    "    if word in w2v_model.wv:\n",
    "        print(f\"\\nSłowa podobne do '{word}':\")\n",
    "        similar = w2v_model.wv.most_similar(word, topn=5)\n",
    "        for s in similar:\n",
    "            print(f\"  {s[0]}: {s[1]:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n'{word}' nie występuje w słowniku\")\n",
    "\n",
    "# Krok 6: Analiza LDA\n",
    "print(\"\\nTrenowanie LDA...\")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=5,\n",
    "    max_iter=10,\n",
    "    learning_method='online',\n",
    "    random_state=42\n",
    ")\n",
    "lda.fit(bow_matrix)\n",
    "\n",
    "# Wyświetlanie tematów\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nGłówne tematy:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Temat #{idx + 1}:\")\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[-10:]]\n",
    "    print(\"   \" + \", \".join(top_words))\n",
    "\n",
    "# Wizualizacja LDA\n",
    "print(\"\\nPrzygotowanie wizualizacji LDA...\")\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_vis = pyLDAvis.sklearn.prepare(lda, bow_matrix, vectorizer, mds='tsne')\n",
    "pyLDAvis.display(lda_vis)\n",
    "\n",
    "# Krok 7: Redukcja wymiarów SVD\n",
    "print(\"\\nRedukcja wymiarów SVD...\")\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "reduced_data = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Krok 8: Wizualizacja 2D\n",
    "print(\"Tworzenie wizualizacji...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], alpha=0.5, cmap='viridis')\n",
    "plt.title('Recenzje filmowe w przestrzeni 2D (SVD)')\n",
    "plt.xlabel('Komponent główny 1')\n",
    "plt.ylabel('Komponent główny 2')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Dodatkowa wizualizacja z tematami\n",
    "dominant_topics = np.argmax(lda.transform(bow_matrix), axis=1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for topic in range(5):\n",
    "    mask = dominant_topics == topic\n",
    "    plt.scatter(\n",
    "        reduced_data[mask, 0], \n",
    "        reduced_data[mask, 1], \n",
    "        alpha=0.6,\n",
    "        label=f'Temat {topic+1}'\n",
    "    )\n",
    "plt.title('Recenzje filmowe z kolorami tematów')\n",
    "plt.xlabel('Komponent główny 1')\n",
    "plt.ylabel('Komponent główny 2')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Chmura słów dla głównych tematów\n",
    "print(\"\\nGenerowanie chmur słów...\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    topic_words = dict(zip(feature_names, lda.components_[i]))\n",
    "    wordcloud = WordCloud(width=300, height=300, background_color='white').generate_from_frequencies(topic_words)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Temat {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Analiza zakończona pomyślnie!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
